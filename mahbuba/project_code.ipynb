{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5099197a-43e3-4dec-b76d-8a5780cc63d5",
   "metadata": {},
   "source": [
    "# DGEB Final Project Notebook: Traditional Models for Convergent Enzyme Classification\n",
    "\n",
    "## Authors: Mahbuba Tasmin & Bryn Reimer  \n",
    "Course: CS 690U — Spring 2025  \n",
    "Date: April 2025\n",
    "\n",
    "## Objective\n",
    "\n",
    "We aim to benchmark the performance of the following traditional approaches:\n",
    "- **Logistic regression** trained on one-hot encoded DNA sequences (512 bp max).\n",
    "- **Logistic regression** trained on $k$-mer count features (e.g., 4-mers).\n",
    "- **BLAST**-based nearest-neighbor prediction using top alignment match from training data.\n",
    "- **Cross-validation** to assess generalization capability on the training set.\n",
    "\n",
    "The remainder of this notebook is structured as follows:\n",
    "1. Dataset loading and preprocessing\n",
    "2. One-hot encoding pipeline + logistic regression\n",
    "3. $k$-mer encoding variant\n",
    "4. Cross-validation evaluation\n",
    "5. BLAST-based baseline\n",
    "6. Comparative performance summary and interpretations\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0283731e-5103-4185-999c-266d4aa7d3ea",
   "metadata": {},
   "source": [
    "## download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "084476fc-0d30-4127-b621-ff6a940a4cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from datasets import DatasetDict\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9e1cd35b-88db-4f78-bf99-64f85efe6787",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {'train': 'data/train-00000-of-00001.parquet', 'test': 'data/test-00000-of-00001.parquet'}\n",
    "df_train = pd.read_parquet(\"hf://datasets/tattabio/convergent_enzymes/\" + splits[\"train\"])\n",
    "df_test = pd.read_parquet(\"hf://datasets/tattabio/convergent_enzymes/\" + splits[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3926d7e-a32d-4649-868b-8aac6390b080",
   "metadata": {},
   "source": [
    "## one-hot encoding based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "89afc527-307e-4b1e-8b9e-6642be8242e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the standard 20 amino acids\n",
    "AMINO_ACIDS = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "aa_to_idx = {aa: i for i, aa in enumerate(AMINO_ACIDS)}\n",
    "\n",
    "def one_hot_encode_protein(seq, max_len=512):\n",
    "    encoding = np.zeros((max_len, len(AMINO_ACIDS)), dtype=np.float32)\n",
    "    for i, aa in enumerate(seq[:max_len]):\n",
    "        if aa in aa_to_idx:\n",
    "            encoding[i, aa_to_idx[aa]] = 1.0\n",
    "    return encoding.flatten()  # Shape: (512 * 20,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "eebb17e3-c8d2-4a4b-a4d7-4b9f89f4204f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    2000.00000\n",
      "mean      415.43800\n",
      "std       276.45399\n",
      "min        55.00000\n",
      "25%       239.00000\n",
      "50%       348.00000\n",
      "75%       514.50000\n",
      "max      3004.00000\n",
      "Name: Sequence, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#check seq lengths\n",
    "seq_lengths = df_train[\"Sequence\"].apply(len)\n",
    "\n",
    "# Summary statistics\n",
    "print(seq_lengths.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "52bc2098-a80d-4173-a6f3-e5246e0e8696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 2000 Test size: 400\n",
      "    Entry    Label                                           Sequence\n",
      "0  A6UQM3  2.7.7.1  MRAFIVGRWQPFHKGHLEIIKKISEEVDEIIIGIGSCQRSHTLTDP...\n",
      "1  A2SS82  2.7.7.1  MRRGLYVGRFQPFHNGHKAVIDGLAEEVDELIIGIGSADISHDIRH...\n",
      "2  A0B5N0  2.7.7.1  MRRGFYIGRFQPYHMGHHLVLEQISREVDEIIVGIGTAQISHTVTD...\n",
      "3  Q9UXN8  2.7.7.1  MRRAFYIGRFQPFHLGHYSLIKDIARDADEVVIGIGSAQKSHEPKN...\n",
      "4  B0R328  2.7.7.1  MTRGFYIGRFQPFHTGHRRVIEQIATEVDELVVGIGSAGDSHSARN...\n"
     ]
    }
   ],
   "source": [
    "## load dataset and convert to pandas\n",
    "# Convert Hugging Face DatasetDict to pandas\n",
    "\n",
    "print(\"Train size:\", len(df_train), \"Test size:\", len(df_test))\n",
    "print(df_train.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "43ac1c78-fa42-4773-bde7-00f9da5ccb24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (2000, 60080)\n"
     ]
    }
   ],
   "source": [
    "## apply encoding to all seq\n",
    "# Apply to training and test data\n",
    "# X_train = np.stack(df_train[\"Sequence\"].map(one_hot_encode_protein))\n",
    "X_train = np.stack(df_train[\"Sequence\"].map(lambda s: one_hot_encode_protein(s, max_len=3004))) # Preserve ~75% of sequence context for most samples\n",
    "\n",
    "# X_test = np.stack(df_test[\"Sequence\"].map(one_hot_encode_protein))\n",
    "X_test = np.stack(df_test[\"Sequence\"].map(lambda s: one_hot_encode_protein(s, max_len=3004)))\n",
    "\n",
    "\n",
    "# Check shape\n",
    "print(\"X_train shape:\", X_train.shape)  # Expect (640, 2048)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3848a15-f6eb-4032-8c6a-6221f471527d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### scikit learn log reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd667c87-e672-4e99-8e23-f3e4c4611553",
   "metadata": {},
   "outputs": [],
   "source": [
    "## encode ec numbers as integer labels\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(df_train[\"Label\"])\n",
    "y_test = le.transform(df_test[\"Label\"])\n",
    "\n",
    "# Optional: Show label classes\n",
    "print(\"Classes:\", le.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb9ac30-deff-46d6-98f7-5b3f1add0043",
   "metadata": {},
   "outputs": [],
   "source": [
    "## train log reg\n",
    "clf = LogisticRegression(\n",
    "    solver='saga',\n",
    "    multi_class='multinomial',\n",
    "    penalty='l2',\n",
    "    C=0.5,                 # stronger regularization (faster convergence)\n",
    "    max_iter=300,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122d232a-b493-44ac-a472-885aa61ee5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## evaluate model\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Macro F1-score:\", f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "# Optional: Full classification report\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6a4251-22d0-44f3-9ad9-99f3e0391b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## save encoding data for reuse \n",
    "\n",
    "np.savez(\"convenz_onehot_data.npz\", X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test, classes=le.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95816fcc-87dd-4ef7-815d-ce819cce23b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize model coefficients\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Visualize coefficient heatmap\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(clf.coef_, cmap=\"coolwarm\", center=0, xticklabels=False)\n",
    "plt.xlabel(\"One-hot features (512 × 4)\")\n",
    "plt.ylabel(\"EC Label Index\")\n",
    "plt.title(\"Logistic Regression Weights (One-hot DNA)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4f2a0b-44f6-4bb7-8550-58b294c1d4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(\n",
    "    clf,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    cv=5,\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=-1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fbbdfa-ec18-48fa-bd4f-44e56088841b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"5-fold cross-validated F1 scores:\", scores)\n",
    "print(\"Mean F1:\", scores.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa313a0-7ce7-494b-9d2d-0fb6a2f3fb03",
   "metadata": {},
   "source": [
    "## pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "794532a7-89de-4037-add1-8822365f882b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "74d492eb-49dd-4d1a-adff-e1f21ed5f8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## pytoch for faster training\n",
    "\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_train_enc = le.fit_transform(y_train)\n",
    "y_test_enc = le.transform(y_test)\n",
    "\n",
    "# Convert to tensors and move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train_enc, dtype=torch.long).to(device)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test_enc, dtype=torch.long).to(device)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ebf8ed9a-77c9-4706-bd6d-31457b8f17fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "model = LogisticRegression(input_dim=X_train.shape[1], num_classes=len(le.classes_)).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "414b03f2-9b7f-4c9f-89fa-5c012999df41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 186.2881\n",
      "Epoch 2, Loss: 26.3160\n",
      "Epoch 3, Loss: 7.5099\n",
      "Epoch 4, Loss: 3.8692\n",
      "Epoch 5, Loss: 2.2306\n",
      "Epoch 6, Loss: 1.4889\n",
      "Epoch 7, Loss: 1.0753\n",
      "Epoch 8, Loss: 0.7922\n",
      "Epoch 9, Loss: 0.6150\n",
      "Epoch 10, Loss: 0.4854\n",
      "Epoch 11, Loss: 0.3989\n",
      "Epoch 12, Loss: 0.3336\n",
      "Epoch 13, Loss: 0.2819\n",
      "Epoch 14, Loss: 0.2467\n",
      "Epoch 15, Loss: 0.2143\n",
      "Epoch 16, Loss: 0.1892\n",
      "Epoch 17, Loss: 0.1681\n",
      "Epoch 18, Loss: 0.1513\n",
      "Epoch 19, Loss: 0.1500\n",
      "Epoch 20, Loss: 0.1266\n",
      "Epoch 21, Loss: 0.1143\n",
      "Epoch 22, Loss: 0.1048\n",
      "Epoch 23, Loss: 0.0969\n",
      "Epoch 24, Loss: 0.0905\n",
      "Epoch 25, Loss: 0.0829\n",
      "Epoch 26, Loss: 0.0804\n",
      "Epoch 27, Loss: 0.0728\n",
      "Epoch 28, Loss: 0.0700\n",
      "Epoch 29, Loss: 0.0651\n",
      "Epoch 30, Loss: 0.0612\n",
      "Epoch 31, Loss: 0.0567\n",
      "Epoch 32, Loss: 0.0544\n",
      "Epoch 33, Loss: 0.0512\n",
      "Epoch 34, Loss: 0.0498\n",
      "Epoch 35, Loss: 0.0458\n",
      "Epoch 36, Loss: 0.0449\n",
      "Epoch 37, Loss: 0.0440\n",
      "Epoch 38, Loss: 0.0406\n",
      "Epoch 39, Loss: 0.0378\n",
      "Epoch 40, Loss: 0.0362\n",
      "Epoch 41, Loss: 0.0360\n",
      "Epoch 42, Loss: 0.0335\n",
      "Epoch 43, Loss: 0.0324\n",
      "Epoch 44, Loss: 0.0319\n",
      "Epoch 45, Loss: 0.0297\n",
      "Epoch 46, Loss: 0.0288\n",
      "Epoch 47, Loss: 0.0274\n",
      "Epoch 48, Loss: 0.0264\n",
      "Epoch 49, Loss: 0.0251\n",
      "Epoch 50, Loss: 0.0246\n",
      "Epoch 51, Loss: 0.0244\n",
      "Epoch 52, Loss: 0.0226\n",
      "Epoch 53, Loss: 0.0217\n",
      "Epoch 54, Loss: 0.0212\n",
      "Epoch 55, Loss: 0.0209\n",
      "Epoch 56, Loss: 0.0199\n",
      "Epoch 57, Loss: 0.0191\n",
      "Epoch 58, Loss: 0.0188\n",
      "Epoch 59, Loss: 0.0181\n",
      "Epoch 60, Loss: 0.0173\n",
      "Epoch 61, Loss: 0.0169\n",
      "Epoch 62, Loss: 0.0163\n",
      "Epoch 63, Loss: 0.0162\n",
      "Epoch 64, Loss: 0.0154\n",
      "Epoch 65, Loss: 0.0150\n",
      "Epoch 66, Loss: 0.0146\n",
      "Epoch 67, Loss: 0.0141\n",
      "Epoch 68, Loss: 0.0138\n",
      "Epoch 69, Loss: 0.0133\n",
      "Epoch 70, Loss: 0.0129\n",
      "Epoch 71, Loss: 0.0127\n",
      "Epoch 72, Loss: 0.0127\n",
      "Epoch 73, Loss: 0.0118\n",
      "Epoch 74, Loss: 0.0115\n",
      "Epoch 75, Loss: 0.0113\n",
      "Epoch 76, Loss: 0.0111\n",
      "Epoch 77, Loss: 0.0109\n",
      "Epoch 78, Loss: 0.0108\n",
      "Epoch 79, Loss: 0.0106\n",
      "Epoch 80, Loss: 0.0101\n",
      "Epoch 81, Loss: 0.0097\n",
      "Epoch 82, Loss: 0.0095\n",
      "Epoch 83, Loss: 0.0094\n",
      "Epoch 84, Loss: 0.0091\n",
      "Epoch 85, Loss: 0.0087\n",
      "Epoch 86, Loss: 0.0086\n",
      "Epoch 87, Loss: 0.0084\n",
      "Epoch 88, Loss: 0.0083\n",
      "Epoch 89, Loss: 0.0080\n",
      "Epoch 90, Loss: 0.0077\n",
      "Epoch 91, Loss: 0.0076\n",
      "Epoch 92, Loss: 0.0076\n",
      "Epoch 93, Loss: 0.0072\n",
      "Epoch 94, Loss: 0.0071\n",
      "Epoch 95, Loss: 0.0070\n",
      "Epoch 96, Loss: 0.0070\n",
      "Epoch 97, Loss: 0.0067\n",
      "Epoch 98, Loss: 0.0066\n",
      "Epoch 99, Loss: 0.0064\n",
      "Epoch 100, Loss: 0.0062\n",
      "Accuracy: 0.015\n",
      "Macro F1: 0.0125\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(X_test_tensor)\n",
    "    preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "    y_true = y_test_tensor.cpu().numpy()\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_true, preds))\n",
    "print(\"Macro F1:\", f1_score(y_true, preds, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0300f8ab-97ef-45b7-a4c5-e92e3500daaa",
   "metadata": {},
   "source": [
    "## K-mer Count Feature Variant (e.g., 3-mers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ea98b70e-01a6-42c8-83b9-6c7d6b40f6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import product\n",
    "\n",
    "def kmer_features_protein(seq, k=3):\n",
    "    # Define k-mer vocabulary from 20 amino acids\n",
    "    vocab = [''.join(p) for p in product(AMINO_ACIDS, repeat=k)]\n",
    "    counts = Counter([seq[i:i+k] for i in range(len(seq)-k+1)])\n",
    "    return np.array([counts.get(kmer, 0) for kmer in vocab], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d28c5245-adf5-4813-8f2e-fbf0d549aab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of k-mer feature matrix: (2000, 8000)\n"
     ]
    }
   ],
   "source": [
    "X_train_kmer = np.stack(df_train[\"Sequence\"].map(lambda s: kmer_features_protein(s, k=3)))\n",
    "X_test_kmer = np.stack(df_test[\"Sequence\"].map(lambda s: kmer_features_protein(s, k=3)))\n",
    "\n",
    "print(\"Shape of k-mer feature matrix:\", X_train_kmer.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "322583b8-c61d-45b5-b908-34ded8ca9ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(input_dim=X_train_kmer.shape[1], num_classes=len(le.classes_)).to(device)\n",
    "X_train_tensor = torch.tensor(X_train_kmer, dtype=torch.float32).to(device)\n",
    "X_test_tensor = torch.tensor(X_test_kmer, dtype=torch.float32).to(device)\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a13bc4d9-d04b-487c-ae53-1276eb7ebe0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 179.8290\n",
      "Epoch 2, Loss: 31.1953\n",
      "Epoch 3, Loss: 10.5838\n",
      "Epoch 4, Loss: 5.6556\n",
      "Epoch 5, Loss: 3.5489\n",
      "Epoch 6, Loss: 2.4338\n",
      "Epoch 7, Loss: 1.7198\n",
      "Epoch 8, Loss: 1.3128\n",
      "Epoch 9, Loss: 1.0092\n",
      "Epoch 10, Loss: 0.8111\n",
      "Epoch 11, Loss: 0.6477\n",
      "Epoch 12, Loss: 0.5384\n",
      "Epoch 13, Loss: 0.4605\n",
      "Epoch 14, Loss: 0.3873\n",
      "Epoch 15, Loss: 0.3373\n",
      "Epoch 16, Loss: 0.3029\n",
      "Epoch 17, Loss: 0.2632\n",
      "Epoch 18, Loss: 0.2355\n",
      "Epoch 19, Loss: 0.2127\n",
      "Epoch 20, Loss: 0.1911\n",
      "Epoch 21, Loss: 0.1780\n",
      "Epoch 22, Loss: 0.1640\n",
      "Epoch 23, Loss: 0.1483\n",
      "Epoch 24, Loss: 0.1356\n",
      "Epoch 25, Loss: 0.1276\n",
      "Epoch 26, Loss: 0.1190\n",
      "Epoch 27, Loss: 0.1104\n",
      "Epoch 28, Loss: 0.1068\n",
      "Epoch 29, Loss: 0.0974\n",
      "Epoch 30, Loss: 0.0909\n",
      "Epoch 31, Loss: 0.0866\n",
      "Epoch 32, Loss: 0.0825\n",
      "Epoch 33, Loss: 0.0761\n",
      "Epoch 34, Loss: 0.0728\n",
      "Epoch 35, Loss: 0.0698\n",
      "Epoch 36, Loss: 0.0662\n",
      "Epoch 37, Loss: 0.0623\n",
      "Epoch 38, Loss: 0.0598\n",
      "Epoch 39, Loss: 0.0567\n",
      "Epoch 40, Loss: 0.0554\n",
      "Epoch 41, Loss: 0.0522\n",
      "Epoch 42, Loss: 0.0494\n",
      "Epoch 43, Loss: 0.0476\n",
      "Epoch 44, Loss: 0.0454\n",
      "Epoch 45, Loss: 0.0454\n",
      "Epoch 46, Loss: 0.0426\n",
      "Epoch 47, Loss: 0.0425\n",
      "Epoch 48, Loss: 0.0389\n",
      "Epoch 49, Loss: 0.0374\n",
      "Epoch 50, Loss: 0.0382\n",
      "Epoch 51, Loss: 0.0347\n",
      "Epoch 52, Loss: 0.0335\n",
      "Epoch 53, Loss: 0.0322\n",
      "Epoch 54, Loss: 0.0316\n",
      "Epoch 55, Loss: 0.0301\n",
      "Epoch 56, Loss: 0.0292\n",
      "Epoch 57, Loss: 0.0287\n",
      "Epoch 58, Loss: 0.0272\n",
      "Epoch 59, Loss: 0.0268\n",
      "Epoch 60, Loss: 0.0258\n",
      "Epoch 61, Loss: 0.0252\n",
      "Epoch 62, Loss: 0.0244\n",
      "Epoch 63, Loss: 0.0235\n",
      "Epoch 64, Loss: 0.0228\n",
      "Epoch 65, Loss: 0.0222\n",
      "Epoch 66, Loss: 0.0214\n",
      "Epoch 67, Loss: 0.0208\n",
      "Epoch 68, Loss: 0.0208\n",
      "Epoch 69, Loss: 0.0196\n",
      "Epoch 70, Loss: 0.0193\n",
      "Epoch 71, Loss: 0.0184\n",
      "Epoch 72, Loss: 0.0184\n",
      "Epoch 73, Loss: 0.0177\n",
      "Epoch 74, Loss: 0.0172\n",
      "Epoch 75, Loss: 0.0168\n",
      "Epoch 76, Loss: 0.0169\n",
      "Epoch 77, Loss: 0.0157\n",
      "Epoch 78, Loss: 0.0154\n",
      "Epoch 79, Loss: 0.0149\n",
      "Epoch 80, Loss: 0.0148\n",
      "Epoch 81, Loss: 0.0141\n",
      "Epoch 82, Loss: 0.0140\n",
      "Epoch 83, Loss: 0.0137\n",
      "Epoch 84, Loss: 0.0132\n",
      "Epoch 85, Loss: 0.0132\n",
      "Epoch 86, Loss: 0.0126\n",
      "Epoch 87, Loss: 0.0124\n",
      "Epoch 88, Loss: 0.0119\n",
      "Epoch 89, Loss: 0.0117\n",
      "Epoch 90, Loss: 0.0115\n",
      "Epoch 91, Loss: 0.0115\n",
      "Epoch 92, Loss: 0.0109\n",
      "Epoch 93, Loss: 0.0109\n",
      "Epoch 94, Loss: 0.0107\n",
      "Epoch 95, Loss: 0.0102\n",
      "Epoch 96, Loss: 0.0099\n",
      "Epoch 97, Loss: 0.0096\n",
      "Epoch 98, Loss: 0.0096\n",
      "Epoch 99, Loss: 0.0093\n",
      "Epoch 100, Loss: 0.0090\n",
      "Accuracy: 0.0075\n",
      "Macro F1: 0.002572320841551611\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(X_test_tensor)\n",
    "    preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "    y_true = y_test_tensor.cpu().numpy()\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_true, preds))\n",
    "print(\"Macro F1:\", f1_score(y_true, preds, average='macro'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5aceca-d174-420a-8f6d-1320dd0b36e6",
   "metadata": {},
   "source": [
    "## blast based evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f21a41d9-0750-40cd-821d-d24b834229b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#step 1: write sequences to fasta\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.Seq import Seq\n",
    "from Bio import SeqIO\n",
    "\n",
    "# Write training sequences\n",
    "train_records = [\n",
    "    SeqRecord(Seq(seq), id=entry, description=label)\n",
    "    for seq, entry, label in zip(df_train[\"Sequence\"], df_train[\"Entry\"], df_train[\"Label\"])\n",
    "]\n",
    "SeqIO.write(train_records, \"train_set.fasta\", \"fasta\")\n",
    "\n",
    "# Write test sequences\n",
    "test_records = [\n",
    "    SeqRecord(Seq(seq), id=entry, description=label)\n",
    "    for seq, entry, label in zip(df_test[\"Sequence\"], df_test[\"Entry\"], df_test[\"Label\"])\n",
    "]\n",
    "SeqIO.write(test_records, \"test_set.fasta\", \"fasta\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1aa33818-94b4-4211-88cc-f588d9bcd923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #run this from terminal and save the blast results.txt \n",
    "# #step 2 : create BLAST DB\n",
    "# !makeblastdb -in train_set.fasta -dbtype prot -out train_db\n",
    "# # step 3- run blast\n",
    "# !blastp -query test_set.fasta -db train_db -outfmt \"6 qseqid sseqid pident length\" -out blast_results.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9739b0df-bdca-4512-b666-e6d627aae311",
   "metadata": {},
   "outputs": [],
   "source": [
    "## parse blast results and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c39ea1fb-fbee-455f-8b01-8f9bf6fceec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[[\"Entry\", \"Label\"]].to_csv(\"train_metadata.csv\", index=False)\n",
    "df_test[[\"Entry\", \"Label\"]].to_csv(\"test_metadata.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5909c972-f81c-4ac3-874e-bdfa6cafefd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLAST Accuracy: 0.001869741352446245\n",
      "BLAST Macro F1: 0.0007941176470588235\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load BLAST results\n",
    "blast_df = pd.read_csv(\"blast_results.txt\", sep=\"\\t\", header=None,\n",
    "                       names=[\"query\", \"subject\", \"pident\", \"length\", \"bitscore\"])\n",
    "\n",
    "# Load EC label mapping\n",
    "df_train = pd.read_csv(\"train_metadata.csv\")  # Contains Entry and Label\n",
    "df_test = pd.read_csv(\"test_metadata.csv\")\n",
    "\n",
    "entry_to_label = dict(zip(df_train[\"Entry\"], df_train[\"Label\"]))\n",
    "true_labels = dict(zip(df_test[\"Entry\"], df_test[\"Label\"]))\n",
    "\n",
    "# Predict using top BLAST hit\n",
    "blast_df[\"pred_label\"] = blast_df[\"subject\"].map(entry_to_label)\n",
    "blast_df[\"true_label\"] = blast_df[\"query\"].map(true_labels)\n",
    "\n",
    "# Drop NA (if any query had no match)\n",
    "blast_df = blast_df.dropna(subset=[\"pred_label\", \"true_label\"])\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "y_true = blast_df[\"true_label\"]\n",
    "y_pred = blast_df[\"pred_label\"]\n",
    "\n",
    "print(\"BLAST Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "print(\"BLAST Macro F1:\", f1_score(y_true, y_pred, average='macro'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "645b7e97-66bd-4bf2-9ef3-6e9270e8e88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 test sequences had no BLAST hits.\n"
     ]
    }
   ],
   "source": [
    "## check for missing predictions\n",
    "missed = set(df_test[\"Entry\"]) - set(blast_df[\"query\"])\n",
    "print(f\"{len(missed)} test sequences had no BLAST hits.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47af4c3b-5c74-46f8-b5fa-b9266ca4c239",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:esmfold]",
   "language": "python",
   "name": "conda-env-esmfold-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
